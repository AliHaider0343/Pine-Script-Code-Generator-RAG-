from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from operator import itemgetter
from typing import Dict, List, Optional, Sequence
from langchain.schema.embeddings import Embeddings
from langchain.schema.retriever import BaseRetriever
from pydantic import BaseModel
from langchain.schema.language_model import BaseLanguageModel
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.messages import AIMessage, HumanMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
from langchain.schema.runnable import (Runnable, RunnableBranch,
                                       RunnableLambda, RunnableMap)
from langchain.schema import Document
from langchain_google_genai import ChatGoogleGenerativeAI

import os
import openai

os.environ['OPENAI_API_KEY']="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"
os.environ['GOOGLE_API_KEY'] = 'AIzaSyAJjb0Koe8IdFWQB8jwaVTrwelav20wkMY'

embedding_function = OpenAIEmbeddings()
RESPONSE_TEMPLATE = """\
You are Powerful Pine Script Code Generator Powered by ChartFi.\
Generate a Pine Script in Version 5 you are given the user Query and well as the Most relevant Scripts and ts description.
you are required to use your own knowledge as well as Provided Context to Generate a Pine Script Code that full flss the user Requirements.
user might Ask to Convert Version 4 to Version 5 or vice Versa do that accordingly.
Anything between the following `context`  html blocks is retrieved from a Most Relevant Scripts knowledge \
bank, not part of the conversation with the user. 

<context>
    {context} 
<context/>

Responce Strcuture:
    -Provide the Script Title and Theme
    -Provide the Script in the Code Block.
    -Write a Working and Procedure of the Code
    -at end entitle that "The Script is generated by ChartFi.io"

MUST REMEMBER: 
Do not answer question on your own Must Refer to the Context If there is no relevant information within the context, just say "Sorry for Inconvenice, i dont have any Information about it in my Digital Brain."
Don't try to make up an answer. 
Anything between the preceding 'context' html blocks is retrieved from Most Relevant Scripts knowledge bank, not part of the conversation with the user.
You are a helpful AI Assistant for Pine SCript 5 Code Generation. Respond to the Greeting Messages Properly.

"""
REPHRASE_TEMPLATE = """\
Given the following conversation and a follow up question, rephrase the follow up \
question to be a standalone question.
Chat History:
{chat_history}
Follow Up Input: {question}
Standalone Question:"""

refrence_docuemnts_sources=[]
class ChatRequest(BaseModel):
    question: str
    chat_history: Optional[List[Dict[str, str]]]

def create_retriever_chain(llm: BaseLanguageModel, retriever: BaseRetriever) -> Runnable:
    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(REPHRASE_TEMPLATE)
    condense_question_chain = (CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()).with_config(
            run_name="CondenseQuestion", )
    conversation_chain = condense_question_chain | retriever

    return RunnableBranch(
            (
                RunnableLambda(lambda x: bool(x.get("chat_history"))).with_config(
                    run_name="HasChatHistoryCheck"
                ),
                conversation_chain.with_config(run_name="RetrievalChainWithHistory"),
            ),
            (
                    RunnableLambda(itemgetter("question")).with_config(
                        run_name="Itemgetter:question"
                    )
                    | retriever
            ).with_config(run_name="RetrievalChainWithNoHistory"),
        ).with_config(run_name="RouteDependingOnChatHistory")

def format_docs(docs: Sequence[Document]) -> str:
    formatted_docs = []
    for i, doc in enumerate(docs):
        refrence_docuemnts_sources.append({'Context-Information': doc.page_content})
        doc_string = f"<doc id='{i}'>{doc.page_content}</doc>"
        formatted_docs.append(doc_string)
    return "\n".join(formatted_docs)

def serialize_history(request: ChatRequest):
    chat_history = request["chat_history"] or []
    converted_chat_history = []
    for message in chat_history:
        if message.get("human") is not None:
            converted_chat_history.append(HumanMessage(content=message["human"]))
        if message.get("ai") is not None:
            converted_chat_history.append(AIMessage(content=message["ai"]))
    return converted_chat_history

def create_chain(llm: BaseLanguageModel,retriever: BaseRetriever,) -> Runnable:
    retriever_chain = create_retriever_chain(
            llm,
            retriever,
        ).with_config(run_name="FindDocs")
    _context = RunnableMap(
            {
                "context": retriever_chain | format_docs,
                "question": itemgetter("question"),
                "chat_history": itemgetter("chat_history"),
            }
        ).with_config(run_name="RetrieveDocs")
    prompt = ChatPromptTemplate.from_messages(
            [
                ("system", RESPONSE_TEMPLATE),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{question}"),
            ]
        )

    response_synthesizer = (prompt | llm | StrOutputParser()).with_config(
            run_name="GenerateResponse",
        )
    return (
                {
                    "question": RunnableLambda(itemgetter("question")).with_config(
                        run_name="Itemgetter:question"
                    ),
                    "chat_history": RunnableLambda(serialize_history).with_config(
                        run_name="SerializeHistory"
                    ),
                }
                | _context
                | response_synthesizer
        )

def Get_Conversation_chain(query,chat_history,model="gpt-4"):
    chroma_db = Chroma(persist_directory=f"./Tradng-View-Indicators-chroma_db", embedding_function=OpenAIEmbeddings())
    #retriever = chroma_db.as_retriever(search_kwargs=dict(k=3))
    retriever = chroma_db.as_retriever(search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.6})

    if model=='gemini-pro':
        llm = ChatGoogleGenerativeAI(model="gemini-pro", convert_system_message_to_human=True)
    else:
        llm = ChatOpenAI(
        model=model,
        streaming=True,
        temperature=0,)

    answer_chain = create_chain(
        llm,
        retriever,
    )
    answer = answer_chain.invoke( {"question": query, "chat_history":chat_history})

    return answer,refrence_docuemnts_sources